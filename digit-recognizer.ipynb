{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Import libraries\nimport os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2024-11-11T00:41:24.332166Z","iopub.execute_input":"2024-11-11T00:41:24.332722Z","iopub.status.idle":"2024-11-11T00:41:28.949886Z","shell.execute_reply.started":"2024-11-11T00:41:24.332686Z","shell.execute_reply":"2024-11-11T00:41:28.948748Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# 1. Load Data from CSV\ntrain_df = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-11-11T00:50:59.790556Z","iopub.execute_input":"2024-11-11T00:50:59.791464Z","iopub.status.idle":"2024-11-11T00:51:02.577770Z","shell.execute_reply.started":"2024-11-11T00:50:59.791422Z","shell.execute_reply":"2024-11-11T00:51:02.576728Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# 2. Preprocess the Data\nX = train_df.iloc[:, 1:].values  # Features (all columns except the first one)\ny = train_df.iloc[:, 0].values   # Target (first column)\n\n# Optional: Normalize or Standardize the data (important for NN)\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T00:51:04.301571Z","iopub.execute_input":"2024-11-11T00:51:04.301992Z","iopub.status.idle":"2024-11-11T00:51:04.801531Z","shell.execute_reply.started":"2024-11-11T00:51:04.301953Z","shell.execute_reply":"2024-11-11T00:51:04.800673Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# 3. Split the Data into Training and Validation Sets\nX_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.1, random_state=42)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)  # For classification (long tensor)\ny_val_tensor = torch.tensor(y_val, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T00:51:07.417897Z","iopub.execute_input":"2024-11-11T00:51:07.418552Z","iopub.status.idle":"2024-11-11T00:51:07.870164Z","shell.execute_reply.started":"2024-11-11T00:51:07.418510Z","shell.execute_reply":"2024-11-11T00:51:07.869109Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# 4. Create DataLoaders\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n\ntrainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nvalloader = DataLoader(val_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-11T00:51:10.143108Z","iopub.execute_input":"2024-11-11T00:51:10.143860Z","iopub.status.idle":"2024-11-11T00:51:10.155865Z","shell.execute_reply.started":"2024-11-11T00:51:10.143819Z","shell.execute_reply":"2024-11-11T00:51:10.154943Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# 5. Define the Convolutionary Neural Network Model\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        \n        # First Convolutional Layer: input 1 channel, output 32 channels, kernel size 3x3\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n        self.relu1 = nn.ReLU()\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Second Convolutional Layer: input 32 channels, output 64 channels, kernel size 3x3\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.relu2 = nn.ReLU()\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # 64 channels, 7x7 feature map size after pooling\n        self.relu3 = nn.ReLU()\n        self.fc2 = nn.Linear(128, 10)  # Output 10 classes (digits 0-9)\n        \n    def forward(self, x):\n        x = self.pool1(self.relu1(self.conv1(x)))  # Conv1 -> ReLU -> MaxPool1\n        x = self.pool2(self.relu2(self.conv2(x)))  # Conv2 -> ReLU -> MaxPool2\n        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n        x = self.relu3(self.fc1(x))  # Fully connected layer 1 -> ReLU\n        x = self.fc2(x)  # Fully connected layer 2 -> Output layer\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-11-11T00:54:06.869724Z","iopub.execute_input":"2024-11-11T00:54:06.870632Z","iopub.status.idle":"2024-11-11T00:54:06.880285Z","shell.execute_reply.started":"2024-11-11T00:54:06.870590Z","shell.execute_reply":"2024-11-11T00:54:06.879307Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# 6. Initialize Model, Loss Function, and Optimizer\ninput_dim = X_train.shape[1]  # Number of features in the dataset\noutput_dim = len(train_df.iloc[:, 0].unique())  # Number of classes for classification (unique labels)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SimpleNN(input_dim, output_dim).to(device)  # Move model to GPU\ncriterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T00:54:10.278492Z","iopub.execute_input":"2024-11-11T00:54:10.279443Z","iopub.status.idle":"2024-11-11T00:54:10.288411Z","shell.execute_reply.started":"2024-11-11T00:54:10.279400Z","shell.execute_reply":"2024-11-11T00:54:10.287341Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# 7. Training Loop\nnum_epochs = 15\n\nfor epoch in range(num_epochs):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in trainloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()  # Zero the gradients\n        outputs = model(inputs)  # Forward pass\n        loss = criterion(outputs, labels)  # Compute the loss\n\n        loss.backward()  # Backpropagation\n        optimizer.step()  # Update weights\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)  # Get predicted class\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()  # Count correct predictions\n\n    # Print statistics for this epoch\n    epoch_loss = running_loss / len(trainloader)\n    epoch_accuracy = 100 * correct / total\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T00:54:12.926232Z","iopub.execute_input":"2024-11-11T00:54:12.927465Z","iopub.status.idle":"2024-11-11T00:54:37.176061Z","shell.execute_reply.started":"2024-11-11T00:54:12.927405Z","shell.execute_reply":"2024-11-11T00:54:37.175008Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch [1/15], Loss: 0.2927, Accuracy: 91.53%\nEpoch [2/15], Loss: 0.1137, Accuracy: 96.56%\nEpoch [3/15], Loss: 0.0755, Accuracy: 97.60%\nEpoch [4/15], Loss: 0.0526, Accuracy: 98.37%\nEpoch [5/15], Loss: 0.0374, Accuracy: 98.80%\nEpoch [6/15], Loss: 0.0294, Accuracy: 99.07%\nEpoch [7/15], Loss: 0.0236, Accuracy: 99.24%\nEpoch [8/15], Loss: 0.0228, Accuracy: 99.30%\nEpoch [9/15], Loss: 0.0174, Accuracy: 99.46%\nEpoch [10/15], Loss: 0.0239, Accuracy: 99.24%\nEpoch [11/15], Loss: 0.0157, Accuracy: 99.53%\nEpoch [12/15], Loss: 0.0134, Accuracy: 99.62%\nEpoch [13/15], Loss: 0.0272, Accuracy: 99.30%\nEpoch [14/15], Loss: 0.0163, Accuracy: 99.47%\nEpoch [15/15], Loss: 0.0076, Accuracy: 99.76%\n","output_type":"stream"}]},{"cell_type":"code","source":"# 8. Evaluation on Test Set\nmodel.eval()  # Set the model to evaluation mode\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():  # Disable gradient calculation during evaluation\n    for inputs, labels in valloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ntest_accuracy = 100 * correct / total\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-11-11T00:54:40.487310Z","iopub.execute_input":"2024-11-11T00:54:40.488070Z","iopub.status.idle":"2024-11-11T00:54:40.574078Z","shell.execute_reply.started":"2024-11-11T00:54:40.488028Z","shell.execute_reply":"2024-11-11T00:54:40.573169Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Test Accuracy: 97.05%\n","output_type":"stream"}]},{"cell_type":"code","source":"# 9. Output the submission file\ntest_df= pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")#load the test data\ntest_scaled=scaler.transform(test_df)#Scale the data\ntest_tensor= torch.tensor(test_scaled, dtype=torch.float32) #Convert the into tensor\ntest_tensor = test_tensor.to(device)\n\nwith torch.no_grad():  # Disable gradient calculations (no need for backprop during inference)\n    outputs = model(test_tensor)  # Get raw model outputs (logits)\n    _, predicted_classes = torch.max(outputs, 1)  # Get the predicted class labels\n    \n\nsubmissions= pd.DataFrame({'Imageid': range(1, 28001)})\npredictions= predicted_classes = predicted_classes.cpu().numpy()\nsubmissions['Label']= predictions\nsubmissions.to_csv('submission.csv', index=None)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T00:54:43.992937Z","iopub.execute_input":"2024-11-11T00:54:43.993546Z","iopub.status.idle":"2024-11-11T00:54:46.027365Z","shell.execute_reply.started":"2024-11-11T00:54:43.993498Z","shell.execute_reply":"2024-11-11T00:54:46.026360Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n  warnings.warn(\n","output_type":"stream"}]}]}